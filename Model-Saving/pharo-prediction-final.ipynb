{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:28.947121Z",
     "iopub.status.busy": "2024-12-10T02:47:28.946894Z",
     "iopub.status.idle": "2024-12-10T02:47:38.703685Z",
     "shell.execute_reply": "2024-12-10T02:47:38.702766Z",
     "shell.execute_reply.started": "2024-12-10T02:47:28.947096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlbse-datasets/NLBSE_Dataset_Java.csv\n",
      "/kaggle/input/nlbse-datasets/NLBSE_Dataset_Python.csv\n",
      "/kaggle/input/nlbse-datasets/NLBSE_Dataset_Pharo.csv\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datasets import Dataset,load_dataset\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator,TrainerCallback,set_seed\n",
    "import torch\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:38.708147Z",
     "iopub.status.busy": "2024-12-10T02:47:38.707587Z",
     "iopub.status.idle": "2024-12-10T02:47:38.743438Z",
     "shell.execute_reply": "2024-12-10T02:47:38.742525Z",
     "shell.execute_reply.started": "2024-12-10T02:47:38.708119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/nlbse-datasets/NLBSE_Dataset_Pharo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:38.747203Z",
     "iopub.status.busy": "2024-12-10T02:47:38.746539Z",
     "iopub.status.idle": "2024-12-10T02:47:38.759883Z",
     "shell.execute_reply": "2024-12-10T02:47:38.758942Z",
     "shell.execute_reply.started": "2024-12-10T02:47:38.747174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7781 entries, 0 to 7780\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   class             7781 non-null   object\n",
      " 1   comment_sentence  7781 non-null   object\n",
      " 2   labels            7781 non-null   object\n",
      " 3   types             7781 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 243.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:38.761103Z",
     "iopub.status.busy": "2024-12-10T02:47:38.760887Z",
     "iopub.status.idle": "2024-12-10T02:47:40.645879Z",
     "shell.execute_reply": "2024-12-10T02:47:40.645007Z",
     "shell.execute_reply.started": "2024-12-10T02:47:38.761082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhimel6087\u001b[0m (\u001b[33mhimel6087-bangladesh-university-of-engineering-and-techn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"Wandb\")\n",
    "import wandb\n",
    "\n",
    "# Replace YOUR_API_KEY with your actual API key\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:40.648134Z",
     "iopub.status.busy": "2024-12-10T02:47:40.647029Z",
     "iopub.status.idle": "2024-12-10T02:47:40.663692Z",
     "shell.execute_reply": "2024-12-10T02:47:40.662823Z",
     "shell.execute_reply.started": "2024-12-10T02:47:40.648089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6457 entries, 0 to 7780\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   class             6457 non-null   object\n",
      " 1   comment_sentence  6457 non-null   object\n",
      " 2   labels            6457 non-null   object\n",
      " 3   types             6457 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 252.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['comment_sentence'], keep='first', inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:40.665357Z",
     "iopub.status.busy": "2024-12-10T02:47:40.665012Z",
     "iopub.status.idle": "2024-12-10T02:47:40.673702Z",
     "shell.execute_reply": "2024-12-10T02:47:40.672787Z",
     "shell.execute_reply.started": "2024-12-10T02:47:40.665318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with null values in 'comment_sentence':\n",
      "Empty DataFrame\n",
      "Columns: [class, comment_sentence, labels, types]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "null_rows = df[df['comment_sentence'].isnull()]\n",
    "\n",
    "print(\"Rows with null values in 'comment_sentence':\")\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:40.675526Z",
     "iopub.status.busy": "2024-12-10T02:47:40.675191Z",
     "iopub.status.idle": "2024-12-10T02:47:40.691168Z",
     "shell.execute_reply": "2024-12-10T02:47:40.690336Z",
     "shell.execute_reply.started": "2024-12-10T02:47:40.675488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after removing nulls: (6457, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6457 entries, 0 to 7780\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   class             6457 non-null   object\n",
      " 1   comment_sentence  6457 non-null   object\n",
      " 2   labels            6457 non-null   object\n",
      " 3   types             6457 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 252.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.dropna(subset=['comment_sentence'])\n",
    "print(\"DataFrame shape after removing nulls:\", df_cleaned.shape)\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:40.692493Z",
     "iopub.status.busy": "2024-12-10T02:47:40.692194Z",
     "iopub.status.idle": "2024-12-10T02:47:42.174338Z",
     "shell.execute_reply": "2024-12-10T02:47:42.173479Z",
     "shell.execute_reply.started": "2024-12-10T02:47:40.692467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows containing patterns: 9\n"
     ]
    }
   ],
   "source": [
    "pattern = r'https?://\\S+|\\t'\n",
    "rows_with_pattern = df_cleaned.apply(lambda row: row.astype(str).str.contains(pattern).any(), axis=1)\n",
    "\n",
    "# Count rows with patterns\n",
    "num_rows_with_pattern = rows_with_pattern.sum()\n",
    "print(f\"\\nNumber of rows containing patterns: {num_rows_with_pattern}\")\n",
    "\n",
    "# Remove `//` or `*` from all columns\n",
    "df_cleaned = df_cleaned.replace(pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:42.175781Z",
     "iopub.status.busy": "2024-12-10T02:47:42.175424Z",
     "iopub.status.idle": "2024-12-10T02:47:42.211748Z",
     "shell.execute_reply": "2024-12-10T02:47:42.211077Z",
     "shell.execute_reply.started": "2024-12-10T02:47:42.175751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df_cleaned\n",
    "df['combo'] = df['comment_sentence'] +\"  |  \"+  df['class']\n",
    "pharo_dataset = Dataset.from_pandas(df)\n",
    "# Split the dataset into train and validation subsets\n",
    "train_test_split = pharo_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Extract train and validation datasets\n",
    "pharo_train = train_test_split['train']\n",
    "pharo_test = train_test_split['test']\n",
    "pharo_labels = ['Keyimplementationpoints', 'Example', 'Responsibilities', 'Classreferences', 'Intent', 'Keymessages', 'Collaborators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T02:47:42.213092Z",
     "iopub.status.busy": "2024-12-10T02:47:42.212834Z",
     "iopub.status.idle": "2024-12-10T04:21:11.359684Z",
     "shell.execute_reply": "2024-12-10T04:21:11.358827Z",
     "shell.execute_reply.started": "2024-12-10T02:47:42.213066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ee9597a8774040bd3596b06479e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d5bf801a43425c967716fbc51fdd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1292 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b0b0f872a348dba364c84abc7519fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc006b66e24e468d9fcb53d6e37c046c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1292 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_13377/1096639213.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241210_024751-37rfatn8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface/runs/37rfatn8' target=\"_blank\">./results1</a></strong> to <a href='https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface' target=\"_blank\">https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface/runs/37rfatn8' target=\"_blank\">https://wandb.ai/himel6087-bangladesh-university-of-engineering-and-techn/huggingface/runs/37rfatn8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1280' max='1280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1280/1280 1:32:39, Epoch 15/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.232258</td>\n",
       "      <td>0.857923</td>\n",
       "      <td>0.532203</td>\n",
       "      <td>0.656904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.130035</td>\n",
       "      <td>0.935802</td>\n",
       "      <td>0.770847</td>\n",
       "      <td>0.845353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>0.954579</td>\n",
       "      <td>0.869153</td>\n",
       "      <td>0.909865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.064269</td>\n",
       "      <td>0.969985</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.932770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.047081</td>\n",
       "      <td>0.975542</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.960771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.983110</td>\n",
       "      <td>0.947119</td>\n",
       "      <td>0.964779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.031638</td>\n",
       "      <td>0.984061</td>\n",
       "      <td>0.962712</td>\n",
       "      <td>0.973269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.025095</td>\n",
       "      <td>0.982970</td>\n",
       "      <td>0.978305</td>\n",
       "      <td>0.980632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.982216</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.977869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.023612</td>\n",
       "      <td>0.982947</td>\n",
       "      <td>0.976949</td>\n",
       "      <td>0.979939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.985034</td>\n",
       "      <td>0.981695</td>\n",
       "      <td>0.983362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.020770</td>\n",
       "      <td>0.985034</td>\n",
       "      <td>0.981695</td>\n",
       "      <td>0.983362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.988451</td>\n",
       "      <td>0.986441</td>\n",
       "      <td>0.987445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>0.985743</td>\n",
       "      <td>0.984407</td>\n",
       "      <td>0.985075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.987738</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.985389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.987057</td>\n",
       "      <td>0.982373</td>\n",
       "      <td>0.984709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.017641184851527214, 'eval_precision': 0.9884510869565217, 'eval_recall': 0.9864406779661017, 'eval_f1': 0.987444859178826, 'eval_runtime': 29.8517, 'eval_samples_per_second': 43.281, 'eval_steps_per_second': 2.713, 'epoch': 15.944272445820433}\n"
     ]
    }
   ],
   "source": [
    "# Use Hugging Face's default data collator\n",
    "data_collator = default_data_collator\n",
    "num_labels = len(pharo_labels)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"combo\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "tokenized_train = pharo_train.map(tokenize_function, batched=True)\n",
    "tokenized_test = pharo_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert labels to tensors\n",
    "def encode_labels(examples):\n",
    "    if isinstance(examples['labels'], str):\n",
    "        examples[\"labels\"]=examples[\"labels\"].replace(\" \", \",\")\n",
    "        labels = ast.literal_eval(examples['labels'])\n",
    "    else:\n",
    "        labels = examples['labels']\n",
    "    # Convert labels to tensors\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return {'labels': labels}\n",
    "tokenized_train = tokenized_train.map(encode_labels)\n",
    "tokenized_test = tokenized_test.map(encode_labels)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-large\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0.5).astype(int)  # Multi-label threshold\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1.3269664638093366e-05,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=24,\n",
    "    weight_decay= 0.09130339417806095,\n",
    "    logging_dir=\"./logs1\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use validation loss to select the best model\n",
    "    greater_is_better=False,  # Lower validation loss is better\n",
    ")\n",
    "\n",
    "\n",
    "# Custom callback to monitor F1 score and restart training if F1 is zero\n",
    "class RestartIfF1ZeroCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, train_dataset, eval_dataset, tokenizer, data_collator):\n",
    "        self.trainer = trainer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_collator = data_collator\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Check F1 score\n",
    "        if \"eval_f1\" in metrics and metrics[\"eval_f1\"] == 0.0:\n",
    "            print(\"F1 score is zero. Restarting training...\")\n",
    "            \n",
    "            # Reinitialize model\n",
    "            self.trainer.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"roberta-base\",\n",
    "                num_labels=num_labels,\n",
    "                problem_type=\"multi_label_classification\"\n",
    "            ).to(device)\n",
    "\n",
    "            # Restart training\n",
    "            self.trainer.train_dataset = self.train_dataset\n",
    "            self.trainer.eval_dataset = self.eval_dataset\n",
    "            self.trainer.tokenizer = self.tokenizer\n",
    "            self.trainer.data_collator = self.data_collator\n",
    "\n",
    "            self.trainer.train()\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Add custom callback\n",
    "trainer.add_callback(RestartIfF1ZeroCallback(trainer, tokenized_train, tokenized_test, tokenizer, data_collator))\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./roberta-pharo-multi-label\")\n",
    "tokenizer.save_pretrained(\"./roberta-pharo-multi-label\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T04:22:35.656607Z",
     "iopub.status.busy": "2024-12-10T04:22:35.656194Z",
     "iopub.status.idle": "2024-12-10T04:22:35.668794Z",
     "shell.execute_reply": "2024-12-10T04:22:35.667836Z",
     "shell.execute_reply.started": "2024-12-10T04:22:35.656559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def evaluate_roberta(new_dataset, model, tokenizer, labels, batch_size=16, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned RoBERTa-large model on a new dataset.\n",
    "\n",
    "    Args:\n",
    "        new_dataset (dict): A dictionary with 'combo' (input texts) and 'labels' (one-hot encoded labels).\n",
    "        model (transformers.PreTrainedModel): A trained RoBERTa model.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model.\n",
    "        labels (list): List of label categories.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Device to use ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing precision, recall, and F1-score for each label.\n",
    "        float: Average runtime per batch.\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare data\n",
    "    texts = new_dataset['combo']\n",
    "    true_labels = np.array(new_dataset['labels'])\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Split into batches\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform inference in batches\n",
    "    for i in range(num_batches):\n",
    "        batch_input_ids = input_ids[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_attention_mask = attention_mask[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    avg_runtime = (end_time - start_time) / num_batches\n",
    "\n",
    "    # Concatenate predictions\n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Evaluate metrics for each label\n",
    "    metrics = []\n",
    "    for i, label in enumerate(labels):\n",
    "        tp = np.sum((true_labels[:, i] == 1) & (predictions[:, i] == 1))\n",
    "        fp = np.sum((true_labels[:, i] == 0) & (predictions[:, i] == 1))\n",
    "        fn = np.sum((true_labels[:, i] == 1) & (predictions[:, i] == 0))\n",
    "        tn = np.sum((true_labels[:, i] == 0) & (predictions[:, i] == 0))\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        metrics.append({'label': label, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "    # Convert metrics to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df, avg_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T04:33:53.690152Z",
     "iopub.status.busy": "2024-12-10T04:33:53.689771Z",
     "iopub.status.idle": "2024-12-10T04:33:59.287587Z",
     "shell.execute_reply": "2024-12-10T04:33:59.286410Z",
     "shell.execute_reply.started": "2024-12-10T04:33:53.690119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "                     label  precision    recall        f1\n",
      "0  Keyimplementationpoints   0.733333  0.511628  0.602740\n",
      "1                  Example   0.921739  0.890756  0.905983\n",
      "2         Responsibilities   0.585714  0.788462  0.672131\n",
      "3          Classreferences   0.500000  0.750000  0.600000\n",
      "4                   Intent   0.843750  0.900000  0.870968\n",
      "5              Keymessages   0.739130  0.790698  0.764045\n",
      "6            Collaborators   0.454545  0.500000  0.476190\n",
      "Average runtime per batch: 0.2286 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Use the model and tokenizer from the training session\n",
    "trained_model = model  # Replace with your trained model instance\n",
    "trained_tokenizer = tokenizer  # Replace with your tokenizer instance\n",
    "test = load_dataset('NLBSE/nlbse25-code-comment-classification')['pharo_test']\n",
    "# def clean_text(example):\n",
    "#     if \"combo\" in example:  # Replace \"text\" with the relevant column name\n",
    "#         example[\"combo\"] = re.sub(r'https?://\\S+|\\t', '', example[\"combo\"], flags=re.MULTILINE)\n",
    "#         example[\"combo\"] = example[\"combo\"].strip()  # Remove leading/trailing spaces\n",
    "#     return example\n",
    "# test = test.map(clean_text)\n",
    "# Define label names\n",
    "labels =  ['Keyimplementationpoints', 'Example', 'Responsibilities', 'Classreferences', 'Intent', 'Keymessages', 'Collaborators']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics, avg_runtime = evaluate_roberta(test, trained_model, trained_tokenizer, labels)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics)\n",
    "print(f\"Average runtime per batch: {avg_runtime:.4f} seconds\")\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Retrieve API token from Kaggle secrets\n",
    "# import os\n",
    "# huggingface_token = user_secrets.get_secret(\"LLama Token\")\n",
    "\n",
    "# login(token=huggingface_token)\n",
    "# trained_model.push_to_hub(\"NLBSE-Pharo-final\")\n",
    "# trained_tokenizer.push_to_hub(\"NLBSE-Pharo-final\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6180789,
     "sourceId": 10034878,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6247649,
     "sourceId": 10124498,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6247652,
     "sourceId": 10124502,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
