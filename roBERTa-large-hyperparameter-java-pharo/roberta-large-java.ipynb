{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10133105,"sourceType":"datasetVersion","datasetId":6253881},{"sourceId":10140759,"sourceType":"datasetVersion","datasetId":6258902},{"sourceId":10143196,"sourceType":"datasetVersion","datasetId":6260732},{"sourceId":10147451,"sourceType":"datasetVersion","datasetId":6263992}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade datasets\n%pip install --upgrade transformers\n%pip install --upgrade optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:52:51.129049Z","iopub.execute_input":"2024-12-08T00:52:51.129766Z","iopub.status.idle":"2024-12-08T00:53:18.458243Z","shell.execute_reply.started":"2024-12-08T00:52:51.129732Z","shell.execute_reply":"2024-12-08T00:53:18.457019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datasets import Dataset,load_dataset\nimport ast\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator,set_seed\nimport torch\nimport optuna\nimport shutil\nseed = 42\nset_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif torch.cuda.is_available():\n    # Get the number of GPUs available\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\n    \n    # Print out the name of each GPU and memory details\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"Total memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n    \n    # If multiple GPUs are available, use DataParallel for multi-GPU\n    if num_gpus > 1:\n        print(\"Using DataParallel for multi-GPU training.\")\n        # Example: Wrap your model with DataParallel\n    else:\n        print(\"Only one GPU available, using single GPU mode.\")\nelse:\n    print(\"No GPUs available, using CPU.\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:18.46088Z","iopub.execute_input":"2024-12-08T00:53:18.461387Z","iopub.status.idle":"2024-12-08T00:53:18.475317Z","shell.execute_reply.started":"2024-12-08T00:53:18.461314Z","shell.execute_reply":"2024-12-08T00:53:18.474498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/java-dataset/NLBSE_Dataset_Java.csv\")\ntest = load_dataset('NLBSE/nlbse25-code-comment-classification')['java_test']\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:18.4764Z","iopub.execute_input":"2024-12-08T00:53:18.476631Z","iopub.status.idle":"2024-12-08T00:53:20.495525Z","shell.execute_reply.started":"2024-12-08T00:53:18.476602Z","shell.execute_reply":"2024-12-08T00:53:20.494549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)\n# if we are in a kaggle environment we need to use that\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Wandb\")\nimport wandb\n# Replace YOUR_API_KEY with your actual API key\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:20.498222Z","iopub.execute_input":"2024-12-08T00:53:20.498632Z","iopub.status.idle":"2024-12-08T00:53:20.814523Z","shell.execute_reply.started":"2024-12-08T00:53:20.498578Z","shell.execute_reply":"2024-12-08T00:53:20.813563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(subset=['comment_sentence'], keep='first', inplace=True)\ndf.info()\nnull_rows = df[df['comment_sentence'].isnull()]\n\nprint(\"Rows with null values in 'comment_sentence':\")\nprint(null_rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:20.815907Z","iopub.execute_input":"2024-12-08T00:53:20.816283Z","iopub.status.idle":"2024-12-08T00:53:20.83882Z","shell.execute_reply.started":"2024-12-08T00:53:20.816241Z","shell.execute_reply":"2024-12-08T00:53:20.837597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned = df.dropna(subset=['comment_sentence'])\nprint(\"DataFrame shape after removing nulls:\", df_cleaned.shape)\ndf_cleaned.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:20.840133Z","iopub.execute_input":"2024-12-08T00:53:20.840435Z","iopub.status.idle":"2024-12-08T00:53:20.856098Z","shell.execute_reply.started":"2024-12-08T00:53:20.840404Z","shell.execute_reply":"2024-12-08T00:53:20.855128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pattern = r'https?://\\S+|\\t'\nrows_with_pattern = df_cleaned.apply(lambda row: row.astype(str).str.contains(pattern).any(), axis=1)\n\n# Count rows with patterns\nnum_rows_with_pattern = rows_with_pattern.sum()\nprint(f\"\\nNumber of rows containing patterns: {num_rows_with_pattern}\")\n\n# Remove `//` or `*` from all columns\ndf_cleaned = df_cleaned.replace(pattern, '', regex=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:20.857125Z","iopub.execute_input":"2024-12-08T00:53:20.857415Z","iopub.status.idle":"2024-12-08T00:53:22.983373Z","shell.execute_reply.started":"2024-12-08T00:53:20.857358Z","shell.execute_reply":"2024-12-08T00:53:22.98241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df_cleaned\ndf['combo'] = df['comment_sentence'] +\"  |  \"+  df['class']\njava_dataset = Dataset.from_pandas(df)\n# Split the dataset into train and validation subsets\ntrain_test_split = java_dataset.train_test_split(test_size=0.2, seed=42)\n\n# Extract train and validation datasets\njava_train = train_test_split['train']\njava_test = train_test_split['test']\njava_labels = ['summary', 'Ownership', 'Expand', 'usage', 'Pointer', 'deprecation', 'rational']\ndata_collator = default_data_collator\nnum_labels = len(java_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:53:22.98462Z","iopub.execute_input":"2024-12-08T00:53:22.984992Z","iopub.status.idle":"2024-12-08T00:53:23.023099Z","shell.execute_reply.started":"2024-12-08T00:53:22.984959Z","shell.execute_reply":"2024-12-08T00:53:23.022047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize global variables\nbest_loss = float('inf')  # Tracks the best evaluation loss\nbest_model_path = \"./best_model\"  # Directory to store the best model\n\n# Ensure best_model directory does not exist at the start\nif os.path.exists(best_model_path):\n    shutil.rmtree(best_model_path)\n\n# Define a function to clear a directory's contents\ndef clear_directory(directory):\n    \"\"\"Removes all contents inside a directory.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        return\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # Remove file or symbolic link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # Remove directory\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n# Define the evaluation function\ndef evaluate(new_dataset, model, tokenizer, labels, batch_size=16, device='cuda'):\n    # Move model to device\n    model.to(device)\n\n    # Prepare data\n    texts = new_dataset['combo']\n    true_labels = np.array(new_dataset['labels'])\n\n    # Tokenize the inputs\n    inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    # Split into batches\n    num_batches = (len(texts) + batch_size - 1) // batch_size\n    predictions = []\n    start_time = time.time()\n\n    # Perform inference in batches\n    for i in range(num_batches):\n        batch_input_ids = input_ids[i * batch_size: (i + 1) * batch_size]\n        batch_attention_mask = attention_mask[i * batch_size: (i + 1) * batch_size]\n\n        with torch.no_grad():\n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n            logits = outputs.logits\n            preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n            predictions.append(preds)\n\n    end_time = time.time()\n    avg_runtime = (end_time - start_time) / num_batches\n\n    # Concatenate predictions\n    predictions = np.vstack(predictions)\n\n    # Evaluate metrics for each label\n    metrics = []\n    for i, label in enumerate(labels):\n        tp = np.sum((true_labels[:, i] == 1) & (predictions[:, i] == 1))\n        fp = np.sum((true_labels[:, i] == 0) & (predictions[:, i] == 1))\n        fn = np.sum((true_labels[:, i] == 1) & (predictions[:, i] == 0))\n        tn = np.sum((true_labels[:, i] == 0) & (predictions[:, i] == 0))\n\n        precision = tp / (tp + fp) if tp + fp > 0 else 0\n        recall = tp / (tp + fn) if tp + fn > 0 else 0\n        f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n\n        metrics.append({'label': label, 'precision': precision, 'recall': recall, 'f1': f1})\n\n    # Convert metrics to DataFrame\n    metrics_df = pd.DataFrame(metrics)\n    average_f1 = metrics_df['f1'].mean()\n    print(\"Average F1 on test set:\", average_f1)\n    return metrics_df, avg_runtime\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n\n# Define the model initialization function\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        \"FacebookAI/roberta-large\",\n        num_labels=num_labels,\n        problem_type=\"multi_label_classification\",\n    )\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"combo\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_train = java_train.map(tokenize_function, batched=True)\ntokenized_test = java_test.map(tokenize_function, batched=True)\n\n# Convert labels to tensors\ndef encode_labels(examples):\n    if isinstance(examples['labels'], str):\n        examples[\"labels\"] = examples[\"labels\"].replace(\" \", \",\")\n        labels = ast.literal_eval(examples['labels'])\n    else:\n        labels = examples['labels']\n    # Convert labels to tensors\n    labels = torch.tensor(labels, dtype=torch.float32)\n    return {'labels': labels}\n\ntokenized_train = tokenized_train.map(encode_labels)\ntokenized_test = tokenized_test.map(encode_labels)\n\n# Format datasets for PyTorch\ntokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Define compute_metrics function for Trainer\ndef compute_metrics(pred):\n    logits, labels = pred\n    print(f\"Logits Shape: {logits.shape}, Labels Shape: {labels.shape}\")\n    # Apply sigmoid to logits to convert to probabilities\n    probs = 1 / (1 + np.exp(-logits))  # Sigmoid function\n    preds = (probs > 0.5).astype(int)  # Threshold for multi-label classification\n    # Compute metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n    if f1 == 0:\n        print(\"F1 score is zero. Resetting the trial.\")\n        raise optuna.exceptions.TrialPruned()  # Prune the trial to reset it\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Define the Optuna objective function\ndef optuna_objective(trial):\n    global best_loss, best_model_path\n\n    # Define hyperparameter search space\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n\n    # Define unique temporary directories for each trial\n    temp_output_dir = f\"./temp_results_bert_base_trial_{trial.number}\"\n    temp_logs_dir = f\"./temp_logs_bert_base_trial_{trial.number}\"\n\n    # Create temporary directories\n    os.makedirs(temp_output_dir, exist_ok=True)\n    os.makedirs(temp_logs_dir, exist_ok=True)\n\n    # Define TrainingArguments\n    training_args = TrainingArguments(\n        output_dir=temp_output_dir,  # Unique temporary directory for each trial\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=5,\n        weight_decay=weight_decay,\n        logging_dir=temp_logs_dir,\n        logging_steps=100,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=1,  # Keep only the best checkpoint\n    )\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model_init=model_init,  # Ensures a fresh model for each trial\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        data_collator=default_data_collator,\n    )\n\n    # Perform training\n    trainer.train()\n\n    # Evaluate on the validation set\n    eval_results = trainer.evaluate()\n    eval_loss = eval_results[\"eval_loss\"]\n    print(f\"Trial {trial.number} - Evaluation Loss: {eval_loss}\")\n\n    # Check if this trial has the best loss so far\n    if eval_loss < best_loss:\n        best_loss = eval_loss\n        print(f\"Trial {trial.number} achieved the new best loss: {best_loss}. Saving the model.\")\n\n        # Remove existing best_model directory if it exists\n        if os.path.exists(best_model_path):\n            shutil.rmtree(best_model_path)\n\n        # **Save the best model correctly using trainer.save_model**\n        trainer.save_model(best_model_path)\n\n    else:\n        print(f\"Trial {trial.number} did not beat the best loss: {best_loss}.\")\n\n    # Clean up the temporary directories to save space\n    shutil.rmtree(temp_output_dir)\n    shutil.rmtree(temp_logs_dir)\n\n    return eval_loss\n\n# Run Optuna search\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(optuna_objective, n_trials=8)\n\n# Display best hyperparameters\nprint(\"Best Hyperparameters:\", study.best_params)\nprint(\"Best Evaluation Loss:\", study.best_value)\n\nbest_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\nbest_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n\n# (Optional) Evaluate the loaded best model\nmetrics_df, avg_runtime = evaluate(\n    new_dataset=test,\n    model=best_model,\n    tokenizer=best_tokenizer,\n    labels=java_labels,\n    device='cuda' if torch.cuda.is_available() else 'cpu'\n)\n\nprint(\"Metrics of the best model:\")\nprint(metrics_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:11:25.349365Z","iopub.execute_input":"2024-12-08T01:11:25.349758Z","iopub.status.idle":"2024-12-08T01:17:59.824457Z","shell.execute_reply.started":"2024-12-08T01:11:25.349728Z","shell.execute_reply":"2024-12-08T01:17:59.823405Z"}},"outputs":[],"execution_count":null}]}