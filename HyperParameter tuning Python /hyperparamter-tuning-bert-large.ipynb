{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10034878,"sourceType":"datasetVersion","datasetId":6180789}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade datasets\n%pip install --upgrade transformers\n%pip install --upgrade optuna","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datasets import Dataset,load_dataset\nimport ast\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\nimport torch\nimport optuna\nimport shutil\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlbse-25-dataset/NLBSE_Dataset_Python.csv\")\ndf.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)\n# if we are in a kaggle environment we need to use that\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Wandb\")\nimport wandb\n# Replace YOUR_API_KEY with your actual API key\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates(subset=['comment_sentence'], keep='first', inplace=True)\ndf.info()\nnull_rows = df[df['comment_sentence'].isnull()]\n\nprint(\"Rows with null values in 'comment_sentence':\")\nprint(null_rows)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned = df.dropna(subset=['comment_sentence'])\nprint(\"DataFrame shape after removing nulls:\", df_cleaned.shape)\ndf_cleaned.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pattern = r'\\*|//'\nrows_with_pattern = df_cleaned.apply(lambda row: row.astype(str).str.contains(pattern).any(), axis=1)\n\n# Count rows with patterns\nnum_rows_with_pattern = rows_with_pattern.sum()\nprint(f\"\\nNumber of rows containing patterns: {num_rows_with_pattern}\")\n\n# Remove `//` or `*` from all columns\ndf_cleaned = df_cleaned.replace(pattern, '', regex=True)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df_cleaned\ndf['combo'] = df['comment_sentence'] +\"  |  \"+  df['class']\npython_dataset = Dataset.from_pandas(df)\n# Split the dataset into train and validation subsets\ntrain_test_split = python_dataset.train_test_split(test_size=0.2, seed=42)\n\n# Extract train and validation datasets\npython_train = train_test_split['train']\npython_test = train_test_split['test']\npython_labels = ['Usage', 'Parameters', 'DevelopmentNotes', 'Expand', 'Summary']","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use Hugging Face's default data collator\ndata_collator = default_data_collator\nnum_labels = len(python_labels)\n\n# Load model\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(\n        \"google-bert/bert-large-uncased\",\n        num_labels=num_labels,\n        problem_type=\"multi_label_classification\",\n    ).to(device)\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")\n\n# Tokenize dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"combo\"], truncation=True, padding=\"max_length\", max_length=128)\ntokenized_train = python_train.map(tokenize_function, batched=True)\ntokenized_test = python_test.map(tokenize_function, batched=True)\n\n# Convert labels to tensors\ndef encode_labels(examples):\n    if isinstance(examples['labels'], str):\n        examples[\"labels\"]=examples[\"labels\"].replace(\" \", \",\")\n        labels = ast.literal_eval(examples['labels'])\n    else:\n        labels = examples['labels']\n    # Convert labels to tensors\n    labels = torch.tensor(labels, dtype=torch.float32)\n    return {'labels': labels}\n    \ntokenized_train = tokenized_train.map(encode_labels)\ntokenized_test = tokenized_test.map(encode_labels)\n\n# Format datasets for PyTorch\ntokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n\n# train_dataloader = DataLoader(tokenized_train, batch_size=32, shuffle=True)\n# test_dataloader = DataLoader(tokenized_test, batch_size=32, shuffle=False)\ndef clear_directory(directory):\n    \"\"\"Removes all contents inside a directory.\"\"\"\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # Remove file or symbolic link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # Remove directory\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n# Define evaluation metrics\ndef compute_metrics(pred):\n    logits, labels = pred\n    print(f\"Logits Shape: {logits.shape}, Labels Shape: {labels.shape}\")\n    # Apply sigmoid to logits to convert to probabilities\n    probs = 1 / (1 + np.exp(-logits))  # Sigmoid function\n    preds = (probs > 0.5).astype(int)  # Threshold for multi-label classification\n    # Compute metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n    \ndef optuna_objective(trial):\n    temp_dir = \"./temp_results\"\n    if os.path.exists(temp_dir):\n        clear_directory(temp_dir)\n    else:\n        os.makedirs(temp_dir)\n    temp_dir = \"./temp_logs\"\n    if os.path.exists(temp_dir):\n        clear_directory(temp_dir)\n    else:\n        os.makedirs(temp_dir)\n    \n    # Define hyperparameter search space\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n    batch_size = trial.suggest_categorical(\"batch_size\", [4,8,16, 32])\n\n    # Initialize Trainer with current trial parameters\n    training_args = TrainingArguments(\n        output_dir=\"./temp_results_bert_large\",  # Temporary directory for checkpoints\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=8,\n        weight_decay=weight_decay,\n        logging_dir=\"./temp_logs_bert_large\",\n        logging_steps=100,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=3,  # Keep only the latest checkpoint\n    )\n\n    trainer = Trainer(\n        model_init=model_init,  # This makes sure we load the base model at each trial\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_test,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n    )\n\n    # Perform training\n    trainer.train()\n    # Evaluate on the validation set\n    eval_results = trainer.evaluate()\n    print(\"Evaluation Results:\", eval_results)\n    eval_loss = eval_results[\"eval_loss\"]\n    # Optuna will minimize this\n    return eval_loss\n\n# Run Optuna search\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(optuna_objective, n_trials=16)\n\n# Display best hyperparameters\nprint(\"Best Hyperparameters:\", study.best_params)\nprint(\"Best Evaluation Loss:\", study.best_value)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}