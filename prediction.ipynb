{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbpeBi919j-Q",
        "outputId": "2bd24617-b5f0-492d-d2ad-1b551a40404c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "   language                    label  precision    recall        f1\n",
            "0      java                  summary   0.904488  0.881166  0.892675\n",
            "1      java                Ownership   1.000000  1.000000  1.000000\n",
            "2      java                   Expand   0.439252  0.460784  0.449761\n",
            "3      java                    usage   0.921951  0.877030  0.898930\n",
            "4      java                  Pointer   0.806452  0.951087  0.872818\n",
            "5      java              deprecation   0.818182  0.600000  0.692308\n",
            "6      java                 rational   0.268293  0.323529  0.293333\n",
            "7    python                    Usage   0.793388  0.793388  0.793388\n",
            "8    python               Parameters   0.852459  0.812500  0.832000\n",
            "9    python         DevelopmentNotes   0.428571  0.292683  0.347826\n",
            "10   python                   Expand   0.683333  0.640625  0.661290\n",
            "11   python                  Summary   0.688172  0.780488  0.731429\n",
            "12    pharo  Keyimplementationpoints   0.733333  0.511628  0.602740\n",
            "13    pharo                  Example   0.921739  0.890756  0.905983\n",
            "14    pharo         Responsibilities   0.585714  0.788462  0.672131\n",
            "15    pharo          Classreferences   0.500000  0.750000  0.600000\n",
            "16    pharo                   Intent   0.843750  0.900000  0.870968\n",
            "17    pharo              Keymessages   0.739130  0.790698  0.764045\n",
            "18    pharo            Collaborators   0.454545  0.500000  0.476190\n",
            "f1 0.7030428716124176\n",
            "average time 149.50380504131317\n",
            "average gflops 552159.158512719\n",
            "-27.24\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, set_seed\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "from huggingface_hub import login\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "def evaluate_roberta(metrics, new_dataset, model, tokenizer, labels, language, batch_size=32, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data\n",
        "    texts = new_dataset['combo']\n",
        "    true_labels = np.array(new_dataset['labels'])  # shape: (num_samples, num_labels)\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Split into batches\n",
        "    num_samples = len(texts)\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    total_time_across_runs = 0.0\n",
        "    total_flops_across_runs = 0.0\n",
        "\n",
        "\n",
        "    final_predictions = None\n",
        "\n",
        "    for run_idx in range(10):\n",
        "        run_time = 0.0\n",
        "        run_flops = 0.0\n",
        "        run_predictions = []\n",
        "        for i in range(num_batches):\n",
        "            batch_input_ids = input_ids[i * batch_size : (i + 1) * batch_size]\n",
        "            batch_attention_mask = attention_mask[i * batch_size : (i + 1) * batch_size]\n",
        "\n",
        "            # Profile *each batch* within this run\n",
        "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "                         with_flops=True) as p:\n",
        "                with torch.no_grad():\n",
        "                    start_time = time.time()\n",
        "                    outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "                    logits = outputs.logits\n",
        "                    preds = (logits.sigmoid() > 0.5).int().cpu().numpy()\n",
        "                    run_time += time.time() - start_time\n",
        "\n",
        "            run_flops += sum(k.flops for k in p.key_averages() if k.flops is not None)\n",
        "\n",
        "            run_predictions.append(preds)\n",
        "\n",
        "        total_time_across_runs += run_time\n",
        "        total_flops_across_runs += run_flops\n",
        "\n",
        "        final_predictions = np.vstack(run_predictions)\n",
        "\n",
        "    # Average runtime and FLOPs over 10 runs\n",
        "    average_time = total_time_across_runs / 10.0\n",
        "    average_flops = total_flops_across_runs / 10.0\n",
        "\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        tp = np.sum((true_labels[:, i] == 1) & (final_predictions[:, i] == 1))\n",
        "        fp = np.sum((true_labels[:, i] == 0) & (final_predictions[:, i] == 1))\n",
        "        fn = np.sum((true_labels[:, i] == 1) & (final_predictions[:, i] == 0))\n",
        "        tn = np.sum((true_labels[:, i] == 0) & (final_predictions[:, i] == 0))\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics.append({\n",
        "            'language': language,\n",
        "            'label': label,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "\n",
        "    #print(\"Avg runtime in seconds (over 10 runs):\", average_time)\n",
        "    average_GFLOPs = average_flops / 1e9\n",
        "    #print(\"Average GFLOPs (over 10 runs):\", average_GFLOPs)\n",
        "\n",
        "    return average_time, average_GFLOPs\n",
        "\n",
        "# Languages and labels\n",
        "langs = ['java', 'python', 'pharo']\n",
        "labels = {\n",
        "    'java': ['summary', 'Ownership', 'Expand', 'usage', 'Pointer', 'deprecation', 'rational'],\n",
        "    'python': ['Usage', 'Parameters', 'DevelopmentNotes', 'Expand', 'Summary'],\n",
        "    'pharo': ['Keyimplementationpoints', 'Example', 'Responsibilities', 'Classreferences', 'Intent', 'Keymessages', 'Collaborators']\n",
        "}\n",
        "\n",
        "ds = load_dataset('NLBSE/nlbse25-code-comment-classification')\n",
        "metrics = []\n",
        "average_time = 0\n",
        "average_GFLOPS = 0\n",
        "for lan in langs:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"MushfiqurRR/NLBSE-{lan.capitalize()}-final\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(f\"MushfiqurRR/NLBSE-{lan.capitalize()}-final\")\n",
        "    test_data = ds[f'{lan}_test']\n",
        "    labels_data = labels[f'{lan}']\n",
        "    A_T,A_GF = (evaluate_roberta(metrics,test_data,model,tokenizer,labels_data,lan))\n",
        "    average_time+=A_T\n",
        "    average_GFLOPS+= A_GF\n",
        "    torch.cuda.empty_cache()\n",
        "metrics = pd.DataFrame(metrics)\n",
        "metrics.reset_index(drop=True, inplace=True)\n",
        "print(metrics)\n",
        "avg_f1 = metrics['f1'].mean()\n",
        "print(\"f1\",avg_f1)\n",
        "print(\"average time\",average_time)\n",
        "print(\"average gflops\",average_GFLOPS)\n",
        "max_avg_flops = 5000\n",
        "max_avg_runtime = 5\n",
        "def score(avg_f1, avg_runtime, avg_flops):\n",
        "    return (\n",
        "        0.6 * avg_f1 +\n",
        "        0.2 * max(0, ((max_avg_runtime - avg_runtime) / max_avg_runtime)) +\n",
        "        0.2 * max(0, ((max_avg_flops - avg_flops) / max_avg_flops))\n",
        "    )\n",
        "print(round(score(avg_f1, average_time, average_GFLOPS), 2))"
      ]
    }
  ]
}